 # LLaMa-from-scratch

This repository contains the implementation of a LLaMa model from scratch, designed to run inference using pretrained weights. The goal is to create a fully functional LLaMa model and leverage pretrained weights for generating outputs.

## To-Do Features

- ~~**Customizable Model Parameters**~~: Easily adjust model dimensions, layers, heads, and other parameters.
- **RMSNorm Normalization**: Implementation of RMSNorm for stable training and inference.
- **Rotary Embeddings**: Application of rotary embeddings for positional encoding.
- **Self-Attention Mechanism**: Efficient self-attention with key-value caching.
- **FeedForward Network**: Custom feedforward network with silu activation.
- **Transformer Encoder**: Stack of encoder blocks for processing input tokens.

   <span style="color:red">**Under work Right Now**</span>
